# -*- coding: utf-8 -*-
"""
Created on Mon Apr 24 16:30:30 2017

@author: Administrator
tensorflow book cifar-10 model
"""

import cifar10,cifar10_input
import tensorflow as tf
import time
import numpy as np

max_steps = 3000
batch_size = 100
data_dir = r'D:\data'

start_time = time.time()
pics = np.empty()

def variable_with_weight_loss(shape,stddev,wl):
    var = tf.Variable(tf.truncated_normal(shape,stddev = stddev))
    if wl is not None:
        weight_loss = tf.multiply(tf.nn.l2_loss(var),wl,name = 'weight_loss')
        tf.add_to_collection('losses',weight_loss)
    return var
    
cifar10.maybe_download_and_extract()

images_train,labels_train = cifar10_input.distorted_inputs(
    data_dir = data_dir,batch_size = batch_size)

image_test,labels_test = cifar10_input.inputs(eval_data = True,
                                              data_dir = data_dir,
                                              batch_size = batch_size)

image_holder = tf.placeholder(tf.float32,[batch_size,24,24,3])
label_holder = tf.placeholder(tf.float32,[batch_size])

weight1 = variable_with_weight_loss(shape = [5,5,3,64],stddev = 5e-2,wl = 0)
kernel1 = tf.nn.conv2d(image_holder,weight1,[1,1,1,1],padding = 'SAME')
bias1 = tf.Variable(tf.constant(0.0,shape = [64]))
conv1 = tf.nn.relu(tf.nn.bias_add(kernel1,bias1))
pool1 = tf.nn.max_pool(conv1,ksize = [1,3,3,1],strides=[1,2,2,1],padding = 'SAME')
norm1 = tf.nn.lrn(pool1,4,bias = 1.0,alpha = 0.001/9.0,beta = 0.75)

weight2 = variable_with_weight_loss(shape = [5,5,64,64],stddev=5e-2,wl = 0.0)
kernel2 = tf.nn.conv2d(norm1,weight2,[1,1,1,1],padding = 'SAME')
bias2 = tf.Variable(tf.constant(0.1,shape = [64]))
conv2 = tf.nn.relu(tf.nn.bias_add(kernel2,bias2))
norm2 = tf.nn.lrn(conv2,4,bias = 1.0,alpha = 0.001/9.0,beta = 0.75)
pool2 = tf.nn.max_pool(norm2,ksize = [1,3,3,1],strides = [1,2,2,1],padding='SAME')

reshape = tf.reshape(pool2,[batch_size,-1])
dim = reshape.get_shape()[1].value
weight3 = variable_with_weight_loss(shape = [dim,384],stddev=0.04,wl = 0.004)
bias3 = tf.Variable(tf.constant(0.1,shape = [384]))
local3 = tf.nn.relu(tf.matmul(reshape,weight3) + bias3)

weight4 = variable_with_weight_loss(shape = [384,192],stddev=0.04,wl=0.004)
bias4 = tf.Variable(tf.constant(0.1,shape = [192]))
local4 = tf.nn.relu(tf.matmul(local3,weight4) + bias4)

weight5 = variable_with_weight_loss(shape = [192,10],stddev = 1/192.0,wl = 0.0)
bias5 = tf.Variable(tf.constant(0.0,shape = [10]))
logits = tf.add(tf.matmul(local4,weight5),bias5)

def loss(logits,labels):
    labels = tf.cast(labels,tf.int64)
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
        logits = logits,labels = labels,name = 'cross_entropy_per_example')
    cross_entropy_mean = tf.reduce_mean(cross_entropy,
                                        name = 'cross_entropy')
    tf.add_to_collection('losses',cross_entropy_mean)
    return tf.add_n(tf.get_collection('losses'),name = 'total_loss')
    
loss = loss(logits,label_holder)

train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)
top_k_op = tf.nn.in_top_k(logits,label_holder,1)

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()

tf.train.start_queue_runners()

for step in range(max_steps):
    start_time = time.time()
    image_batch, label_batch = sess.run([images_train,labels_train])
    _,loss_value = sess.run([train_op,loss],
                            feed_dict = {image_holder:image_batch,label_holder:label_batch})
    duration = time.time() - start_time
    
    if step % 10 == 0:
        examples_per_sec = batch_size / duration
        sec_per_batch = float(duration)
        
        format_str = ('step %d, loss = %.2f (%.1f examples/sec;%.3f sec/batch)')
        print(format_str % (step,loss_value,examples_per_sec,sec_per_batch))

num_examples = 10000
import math
num_iter = int(math.ceil(num_examples / batch_size))
true_count = 0
total_sample_count = num_iter * batch_size
step = 0
while step < num_iter:
    image_batch,label_batch = sess.run([image_test,labels_test])
    predictions = sess.run([top_k_op],
                           feed_dict = {image_holder:image_batch,
                                        label_holder:label_batch})
    true_count += np.sum(predictions)
    step += 1

precision = true_count / total_sample_count
print('precision @ 1 = %.3f' % precision)












#=============================================
#===== AlexNet =======

from datetime import datetime
import math
import time
import tensorflow as tf

batch_size = 32
num_batches = 100

def print_activations(t):
    print(t.op.name,' ',t.get_shape().as_list())\
    
def inference(images):
    parameters = []
    with tf.name_scope('conv1') as scope:
        kernel = tf.Variable(tf.truncated_normal(shape = [11,11,3,64],
                                                 dtype = tf.float32,
                                                 stddev = 0.1),
                                                 name = 'weights')
        conv = tf.nn.conv2d(images,kernel,[1,4,4,1],padding = 'SAME')
        bias = tf.Variable(tf.constant(0.0,shape = [64],
                                         dtype = tf.float32),
                                         trainable=True,
                                         name = 'bias')
        added = tf.nn.bias_add(conv,bias)
        conv1 = tf.nn.relu(added,name = scope)
        print_activations(conv1)
        parameters += [kernel,bias]
        # option 1
        #lrn1 = tf.nn.lrn(conv1,4,bias = 1.0,alpha = 0.001/9,beta = 0.75,name = 'lrn1')
        #pool1 = tf.nn.max_pool(lrn1,ksize = [1,3,3,1],strides = [1,2,2,1],padding = 'VALID',name = 'pool1')
        # option 2, lrn is now considered not useful in inproving accuracy        
        pool1 = tf.nn.max_pool(conv1,ksize = [1,3,3,1])
        print_activations(pool1)

    with tf.name_scope('conv2') as scope:
        kernel = tf.Variable(tf.truncated_normal([5,5,64,192],
                                                 dtype = tf.float32,
                                                 stddev = 0.1),
                                                 name = 'weights')
        conv = tf.nn.conv2d(pool1,kernel,[1,1,1,1],padding = 'SAME')
        bias = tf.Variable(tf.constant(0.0,shape = [192],
                                       dtype = tf.float32),
                                       trainable = True,
                                       name = 'bias')
        added = tf.nn.bias_add(conv,bias)
        conv2 = tf.nn.relu(added,name = scope)
        parameters += [kernel,bias]
        print_activations(conv2)
        
        lrn2 = tf.nn.lrn(conv2,4,bias = 1.0,alpha = 0.001 / 9,beta = 0.75,name = 'lrn2')
        pool2 = tf.nn.max_pool(lrn2,ksize = [1,3,3,1],strides = [1,2,2,1],
                               padding = 'VALID',name = 'pool2')
        print_activations(pool2)
        
    with tf.name_scope('conv3') as scope:
        kernel = tf.Variable(tf.truncate_normal([3,3,192,384],
                                                dtype = tf.float32,
                                                stddev = 0.1),
                                                name = 'weights')
        conv = tf.nn.conv2d(pool2,kernel,[1,1,1,1],padding = 'SAME')
        bias = tf.Variable(tf.constant(0.0,shape = [384],
                                       dtype = tf.float32),
                                       trainable = True,
                                       name = 'bias')
        added = tf.nn.bias_add(conv,bias)
        conv3 = tf.nn.relu(added,name = scope)
        parameters += [kernel,bias]
        print_activations(conv3)
    
    with tf.name_scope('conv4') as scope:
        kernel = tf.Variable(tf.truncated_normal([3,3,384,256],
                                                 dtype = tf.float32,
                                                 stddev = 0.1),
                                                 name = 'weights')
        conv = tf.nn.conv2d(conv3,kernel,[1,1,1,1],padding = 'SAME')
        bias = tf.Variable(tf.constant(0.0,shape = [256],
                                       dtype = tf.float32),
                                       trainable = True,
                                       name = 'bias')
        added = tf.nn.bias_add(conv,bias)
        conv4 = tf.nn.relu(added,name = scope)
        parameters += [kernel,bias]
        print_activations(conv4)
        
    with tf.name_scope('conv5') as scope:
        kernel = tf.Variable(tf.truncate_normal([3,3,256,256],
                                                dtype = tf.float32,
                                                stddev = 0.1),
                                                name = 'weights')
        conv = tf.nn.conv2d(conv4,kernel,[1,1,1,1],padding = 'SAME')
        bias = tf.Variable(tf.constant(0.0,shape = [256],
                                       dtype = tf.floar32),
                                       trainable = True,
                                       name = 'bias')
        added = tf.nn.bias_add(conv,bias)
        conv5 = tf.nn.relu(added,name = scope)
        parameters += [kernel,bias]
        print_activations(conv5)
        pool5 = tf.nn.max_pool(conv5,ksize = [1,3,3,1],strides = [1,2,2,1],
                               padding = 'VALID',name = 'pool5')
        print_activations(pool5)
        return pool5,parameters
        
def time_tensorflow_run(session,target,info_string):
    num_steps_burn_in = 10
    total_duration = 0.0
    total_duration_squared = 0.0
    for i in range(num_batches + num_steps_burn_in):
        start_time = time.time()
        _ = session.run(target)
        duration = time.time() - start_time
        if i >= num_steps_burn_in:
            if not i % 10:
                print('%s:step %d, duration = %.3f' % 
                    (datetime.now(), i - num_steps_burn_in,duration))
            total_duration  += duration
            total_duration_squared += duration * duration
    mn = total_duration / num_batches
    vr = total_duration_squared / num_batches - mn * mn
    sd = math.sqrt(vr)
    print('%s: %s across %d steps, %.3f +/- %.3f sec / batch' % 
        (datetime.now(), info_string, num_batches, mn, sd))
    
def run_benchmark():
    with tf.Graph.as_default():
        image_size = 224
        images = tf.Variable(tf.random_normal([batch_size,image_size,image_size,3],
                                              dtype = tf.float32,
                                              stddev = 0.1))
        pool5, parameters = inference(images)
        init = tf.global_variables_initializer()
        sess = tf.Session()
        sess.run(init)
        time_tensorflow_run(sess,pool5,"Forward")
        objective = tf.nn.l2_loss(pool5)
        grad = tf.gradients(objective, parameters)
        time_tensorflow_run(sess, grad, "Forward-backward")

run_benchmark()

#======================================================
# VGGNet

from datetime import datetime
import math
import time
import tensorflow as tf

def conv_op(input_op,name,kh,kw,n_out,dh,dw,p):
    n_in = input_op.get_shape()[-1].value
    with tf.name_scope(name) as scope:
        kernel = tf.get_variable(scope + "W",
                                 shape = [kh,kw,n_in,n_out],
                                 dtype = tf.float32,
                                 initializer = tf.contrib.layers.xavier_initializer_conv2d())
        conv = tf.nn.conv2d(input_op,kernel,strides=[1,dh,dw,1],padding='SAME')
        bias_init_val = tf.constant(0.0,shape = [n_out],dtype = tf.float32)
        bias = tf.Variable(bias_init_val,trainable=True,name='b')
        added = tf.nn.bias_add(conv,bias)
        activation = tf.nn.relu(added,name = scope)
        p += [kernel,bias]
        return activation
        
def fc_op(input_op,name,n_out,p):
    n_in = input_op.get_shape()[-1].value
    with tf.name_scope(name) as scope:
        kernel = tf.get_variable(scope + "w",
                                 shape = [n_in,n_out],
                                 dtype = tf.float32,
                                 initializer = tf.contrib.layers.xavier_initializer())
        bias = tf.Variable(tf.constant(0.1,shape = [n_out],dtype = tf.float32),name = 'b')
        activation = tf.nn.relu_layer(input_op,kernel,bias,name = scope)
        p += [kernel,bias]
        return activation
        
def mpool_op(input_op,name,kh,kw,dh,dw):
    return tf.nn.max_pool(input_op,
                          ksize = [1,kh,kw,1],
                          strides = [1,dh,dw,1],
                          padding = 'SAME',
                          name = name)

def inference_op(input_op,keep_prob):
    p = []
    conv1_1 = conv_op(input_op,name = "conv1_1",kh=3,kw=3,n_out=64,dh=1,dw=1,p=p)
    conv1_2 = conv_op(conv1_1,name = "conv1_2",kh=3,kw=3,n_out=64,dh=1,dw=1,p=p)
    pool1 = mpool_op(conv1_2,name = "pool1",kh=2,kw=2,dw=2,dh=2)
    
    conv2_1 = conv_op(pool1,name = 'conv2_1',kh=3,kw=3,n_out=128,dh=1,dw=1,p=p)
    conv2_2 = conv_op(conv2_1,name = 'conv2_2',kh=3,kw=3,n_out=128,dh=1,dw=1,p=p)
    pool2 = mpool_op(conv2_2,name='pool2',kh=2,kw=2,dh=2,dw=2)
    
    conv3_1 = conv_op(pool2,name='conv3_1',kh=3,kw=3,n_out=256,dh=1,dw=1,p=p)
    conv3_2 = conv_op(conv3_1,name='conv3_2',kh=3,kw=3,n_out=256,dh=1,dw=1,p=p)
    conv3_3 = conv_op(conv3_2,name='conv3_3',kh=3,kw=3,n_out=256,dh=1,dw=1,p=p)
    pool3 = mpool_op(conv3_3,name='pool3',kh=2,kw=2,dh=2,dw=2)
    
    conv4_1 = conv_op(pool3,name='conv4_1',kh=3,kw=3,n_out=512,dh=1,dw=1,p=p)
    conv4_2 = conv_op(conv4_1,name='conv4_2',kh=3,kw=3,n_out=512,dh=1,dw=1,p=p)
    conv4_3 = conv_op(conv4_2,name='conv4_3',kh=3,kw=3,n_out=512,dh=1,dw=1,p=p)
    pool4 = mpool_op(conv4_3,name='pool4',kh=2,kw=2,dh=2,dw=2)
    
    conv5_1 = conv_op(pool4,name='conv5_1',kh=3,kw=3,n_out=512,dh=1,dw=1,p=p)
    conv5_2 = conv_op(conv5_1,name='conv5_2',kh=3,kw=3,n_out=512,dh=1,dw=1,p=p)
    conv5_3 = conv_op(conv5_2,name='conv5_3',kh=3,kw=3,n_out=512,dh=1,dw=1,p=p)
    pool5 = mpool_op(conv5_3,name='pool5',kh=2,kw=2,dh=2,dw=2)
    
    shp = pool5.get_shape()
    flattened_shape = shp[1]*shp[2]*shp[3]
    resh_1 = tf.reshape(pool5,[-1,flattened_shape],name='resh_1')
    
    fc6 = fc_op(resh_1,name='fc6',n_out=4096,p=p)
    fc6_drop = tf.nn.dropout(fc6,keep_prob,name='fc6_drop')
    fc7 = fc_op(fc6_drop,name='fc7',n_out=4096,p=p)
    fc7_drop = tf.nn.dropout(fc7,keep_prob,name='fc7_drop')
    fc8 = fc_op(fc7_drop,name='fc8',n_out=1000,p=p)
    softmax = tf.nn.softmax(fc8)
    predictions=tf.argmax(softmax,1)
    return predictions,softmax,fc8,p

def time_tensorflow_run(session,target,feed,info_string):
    num_steps_burn_in = 10
    total_duration = 0.0
    total_duration_squared = 0
    for i in range(num_batches + num_steps_burn_in):
        start_time = time.time()
        _ = session.run(target,feed_dict = feed)
        duration = time.time() - start_time
        if i >= num_steps_burn_in:
            if i % 10 == 0:
                print('%s: step %d, duration = %.3f'%(datetime.now(),i-num_steps_burn_in,duration))
            total_duration += duration
            total_duration_squared += duration * duration
    mn = total_duration / num_batches
    vr = total_duration_squared / num_batches - mn * mn
    sd = math.sqrt(vr)
    print('%s: %s across %d steps, %.3f +/- %.3f sec / batch' %
        (datetime.now(),info_string,num_batches,mn,sd))
    
def benchmark_run():
    with tf.Graph.as_default():
        image_size = 224
        images = tf.Variable(tf.random_normal([batch_size,
                                               image_size,
                                               image_size,3],
                                               dtype = tf.float32,
                                               stddev = 0.1))

        keep_prob = tf.placeholder(tf.float32)
        predictions,softmax,fc8,p = inference_op(images,keep_prob)
        init = tf.global_variables_initializer()
        sess = tf.Session()
        sess.run(init)
        time_tensorflow_run(sess,predictions,{keep_prob:1.0},"Forward")
        objective = tf.nn.l2_loss(fc8)
        grad = tf.gradients(objective,p)
        time_tensorflow_run(sess,grad,{keep_prob:0.5},"Forward-backward")
batch_size = 32
num_batches = 100
benchmark_run()










#=======================================================================
# Inception Net


import tensorflow as tf
slim = tf.contrib.slim
trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0,stddev)

def inception_v3_arg_scope(weight_decay = 0.00004,
                           stddev = 0.1,
                           batch_norm_var_collection = 'moving_vars'):
    batch_norm_params = {
        'decay':0.9997,
        'epsilon':0.001,
        'updates_collections':tf.GraphKeys.UPDATE_OPS,
        'variables_collections':{
            'beta':None,
            'gamma':None,
            'moving_mean':[batch_norm_var_collection],
            'moving_variance':[batch_norm_var_collection]
            }
        }
    with slim.arg_scope([slim.conv2d,slim.fully_connected],
                        weights_regularizer = slim.l2_regularizer(weight_decay)):
        with slim.arg_scope(
            [slim.conv2d],
            weights_initializer = tf.truncated_normal_initializer(stddev = stddev),
            activation_fn = tf.nn.relu,
            normalizer_fn = slim.batch_norm,
            normalizer_params = batch_norm_params) as sc:
            return sc
def inception_v3_base(inputs,scope=None):
    end_points = {}
    with tf.variable_scope(scope,'InceptionV3',[inputs]):
        with slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],
                            stride = 1,padding = 'VALID'):
            net = slim.conv2d(inputs,32,[3,3],stride = 2,scope = 'Conv2d_1a_3x3')
            net = slim.conv2d(net,32,[3,3],scope = 'Conv2d_2a_3x3')
            net = slim.conv2d(net,64,[3,3],padding = 'SAME',scope = 'Conv2d_2b_3x3')
            net = slim.max_pool2d(net,[3,3],stride = 2,scope = 'MaxPool_3a_3x3')
            net = slim.conv2d(net,80,[1,1],scope = 'Conv2d_3b_1x1')
            net = slim.conv2d(net,192,[3,3],scope = 'Conv2d_4a_3x3')
            net = slim.max_pool2d(net,[3,3],stride=2,scope='MaxPool_5a_3x3')
            
        with slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],
                            stride = 1,padding = 'SAME'):
            with tf.variable_scope('Mixed_5b'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,64,[1,1],scope = 'Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,48,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1,64,[5,5],scope = 'Conv2d_0b_5x5')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,64,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,96,[3,3],scope = 'Conv2d_0b_3x3')
                    branch_2 = slim.conv2d(branch_2,96,[3,3],scope = 'Conv2d_0c_3x3')
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,32,[1,1],scope='Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
                
            with tf.variable_scope('Mixed_5c'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,64,[1,1],scope = 'Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,48,[1,1],scope = 'Conv2d_0b_1x1')
                    branch_1 = slim.conv2d(branch_1,64,[5,5],scope = 'Conv2d_0c_5x5')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,64,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,96,[3,3],scope = 'Conv2d_0b_3x3')
                    branch_2 = slim.conv2d(branch_2,96,[3,3],scope = 'Conv2d_0c_3x3')
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.avg_pool2d(net,[3,3],scope = 'AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,64,[1,1],scope = 'Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
                
            with tf.variable_scope('Mixed_5d'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,64,[1,1],scope = 'Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,48,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1,64,[5,5],scope = 'Conv2d_0b_5x5')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,64,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,96,[3,3],scope = 'Conv2d_0b_3x3')
                    branch_2 = slim.conv2d(branch_2,96,[3,3],scope = 'Conv2d_0b_3x3')
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.conv2d(net,[3,3],scope = 'AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,64,[1,1],scope = 'Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
                
            with tf.variable_scope('Mixed_6a'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,384,[3,3],stride=2,
                                           padding = 'VALID',scope = 'Conv2d_1a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,64,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1,96,[3,3],scope = 'Conv2d_0b_3x3')
                    branch_1 = slim.conv2d(branch_1,96,[3,3],stride = 2,
                                           padding = 'VALID',scope = 'Conv2d_1a_1x1')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.max_pool2d(net,[3,3],stride = 2,padding = 'VALID',
                                               scope = 'MaxPool_1a_3x3')
                net = tf.concat([branch_0,branch_1,branch_2],3)
                
            with tf.variable_scope('Mixed_6b'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,192,[1,1],scope = 'Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,128,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1,128,[1,7],scope = 'Conv2d_0b_1x7')
                    branch_1 = slim.conv2d(branch_1,192,[7,1],scope = 'Conv2d_0c_7x1')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,128,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,128,[7,1],scope = 'Conv2d_0b_7x1')
                    branch_2 = slim.conv2d(branch_2,128,[1,7],scope = 'Conv2d_0c_1x7')
                    branch_2 = slim.conv2d(branch_2,128,[7,1],scope = 'Conv2d_0d_7x1')
                    branch_2 = slim.conv2d(branch_2,128,[1,7],scope = 'Conv2d_0e_1x7')
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.avg_pool2d(net,[3,3],scope = 'AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,192,[1,1],scope = 'Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
                
            with tf.variable_scope('Mixed_6c'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,192,[1,1],scope = 'Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,160,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1,160,[1,7],scope = 'Conv2d_0b_1x7')
                    branch_1 = slim.conv2d(branch_1,192,[7,1],scope = 'Conv2d_0c_7x1')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,160,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,160,[7,1],scope = 'Conv2d_0b_7x1')
                    branch_2 = slim.conv2d(branch_2,160,[1,7],scope = 'Conv2d_0c_1x7')
                    branch_2 = slim.conv2d(branch_2,160,[7,1],scope = 'Conv2d_0d_7x1')
                    branch_2 = slim.conv2d(branch_2,192,[1,7],scope = 'Conv2d_0e_1x7')
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.avg_pool2d(net,[3,3],scope = 'AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,192,[1,1],scope = 'Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
                
            with tf.variable_scope('Mixed_6d'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,192,[1,1],scope = 'Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,160,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1,160,[1,7],scope = 'Conv2d_0b_1x7')
                    branch_1 = slim.conv2d(branch_1,192,[7,1],scope = 'Conv2d_0c_7x1')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,160,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,160,[7,1],scope = 'Conv2d_0b_7x1')
                    branch_2 = slim.conv2d(branch_2,160,[1,7],scope = 'Conv2d_0c_1x7')
                    branch_2 = slim.conv2d(branch_2,160,[7,1],scope = 'Conv2d_0d_7x1')
                    branch_2 = slim.conv2d(branch_2,192,[1,7],scope = 'Conv2d_0e_1x7')
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.avg_pool2d(net,[3,3],scope = 'AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,192,[1,1],scope = 'Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
                
            with tf.variable_scope('Mixed_6e'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,192,[1,1],scope = 'Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,160,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1,160,[1,7],scope = 'Conv2d_0b_1x7')
                    branch_1 = slim.conv2d(branch_1,192,[7,1],scope = 'Conv2d_0c_7x1')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,160,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,160,[7,1],scope = 'Conv2d_0b_7x1')
                    branch_2 = slim.conv2d(branch_2,160,[1,7],scope = 'Conv2d_0c_1x7')
                    branch_2 = slim.conv2d(branch_2,160,[7,1],scope = 'Conv2d_0d_7x1')
                    branch_2 = slim.conv2d(branch_2,192,[1,7],scope = 'Conv2d_0e_1x7')
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.avg_pool2d(net,[3,3],scope = 'AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,192,[1,1],scope = 'Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
                
            end_points['Mixed_6e'] = net
            
            with tf.variable_scope('Mixed_7a'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,192,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_0 = slim.conv2d(branch_0,320,[3,3],stride = 2,
                                           padding = 'VALID',scope = 'Conv2d_1a_3x3')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,192,[1,1],scope = 'Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1,192,[1,7],scope = 'Conv2d_0b_1x7')
                    branch_1 = slim.conv2d(branch_1,192,[7,1],scope = 'Conv2d_0c_7x1')
                    branch_1 = slim.conv2d(branch_1,192,[3,3],stride = 2,
                                           padding = 'VALID',scope = 'Conv2d_1a_3x3')
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.max_pool2d(net,[3,3],stride = 2,padding = 'VALID',
                                               scope = 'Conv2d_1a_3x3')
                net = tf.concat([branch_0,branch_1,branch_2],3)
            with tf.variable_scope('Mixed_7b'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,320,[1,1],scope='Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,384,[1,1],scope='Conv2d_0a_1x1')
                    branch_1 = tf.concat([
                        slim.conv2d(branch_1,384,[1,3],scope='Conv2d_0b_1x3'),
                        slim.conv2d(branch_1,384,[3,1],scope='Cvon2d_0b_3x1')],3)
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,448,[1,1],scope='Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,384,[3,3],scope='Conv2d_0b_3x3')
                    branch_2 = tf.concat([
                        slim.conv2d(branch_2,384,[1,3],scope='Conv2d_0c_1x3'),
                        slim.conv2d(branch_2,384,[3,1],scope='Conv2d_0d_3x1')],3)
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,192,[1,1],scope='Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
            with tf.variable_scope('Mixed_7c'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net,320,[1,1],scope='Conv2d_0a_1x1')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net,384,[1,1],scope='Conv2d_0a_1x1')
                    branch_1 = tf.concat([
                        slim.conv2d(branch_1,384,[1,3],scope='Conv2d_0b_1x3'),
                        slim.conv2d(branch_1,384,[3,1],scope='Conv2d_0c_3x1')],3)
                with tf.variable_scope('Branch_2'):
                    branch_2 = slim.conv2d(net,448,[1,1],scope='Conv2d_0a_1x1')
                    branch_2 = slim.conv2d(branch_2,384,[3,3],scope='Conv2d_0b_3x3')
                    branch_2 = tf.concat([
                        slim.conv2d(branch_2,384,[1,3],scope='Conv2d_0c_1x3'),
                        slim.conv2d(branch_2,384,[3,1],scope='Conv2d_0d_3x1')],3)
                with tf.variable_scope('Branch_3'):
                    branch_3 = slim.avg_pool2d(net,[3,3],scope='AvgPool_0a_3x3')
                    branch_3 = slim.conv2d(branch_3,192,[1,1],scope='Conv2d_0b_1x1')
                net = tf.concat([branch_0,branch_1,branch_2,branch_3],3)
            return net,end_points

def inception_v3(inputs,
                 num_classes = 1000,
                 is_training = True,
                 dropout_keep_prob = 0.8,
                 prediction_fn = slim.softmax,
                 spatial_squeeze = True,
                 reuse = None,
                 scope = 'InceptionV3'):
    with tf.variable_scope(scope,'InceptionV3',[inputs,num_classes],reuse = reuse) as scope:
        with slim.arg_scope([slim.batch_norm,slim.dropout],is_training = is_training):
            net, end_points = inception_v3_base(inputs,scope = scope)
        with slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],stride=1,padding='SAME'):
            aug_logits = end_points(['Mixed_6e'])
            with  tf.variable_scope('AugLogits'):
                aug_logits = slim.avg_pool2d(
                    aug_logits,[5,5],stride=3,padding='VALID',
                    scope='AvgPool_1a_5x5')
                aug_logits = slim.conv2d(aug_logits,128,[1,1],
                                         scope = 'Conv2d_1b_1x1')
                aug_logits = slim.conv2d(aug_logits,768,[5,5],
                                         weights_initializer = trunc_normal(0.01),
                                         padding = 'VALID',scope='Conv2d_2a_5x5')
                aug_logits = slim.con2d(aug_logits,num_classes,[1,1],activation_fn=None,
                                        normalizer_fn = None,weights_initializer = trunc_normal(0.001),
                                        scope = 'Conv2d_2b_1x1')
                if spatial_squeeze:
                    aug_logits = tf.squeeze(aug_logits,[1,2],name='SpatialSqueeze')
                end_points['AugLogits'] = aug_logits
            with tf.variable_scope('logits'):
                net = slim.avg_pool2d(net,[8,8],padding = 'VALID',
                                      scope = 'AvgPool_1a_8x8')
                net = slim.dropout(net,keep_prob = dropout_keep_prob,
                                   scope = 'Dropout_1b')
                end_points['PreLogits'] = net
                logits = slim.conv2d(net,num_classes,[1,1],activation_fn = None,
                                     normalizer_fn = None, scope = 'Conv2d_1c_1x1')
                if spatial_squeeze:
                    logits = tf.squeeze(logits,[1,2],name = 'SpatialSqueeze')
            end_points['Logits'] = logits
            end_points['Predictions'] = prediction_fn(logits,scope = 'Predictions')
        return logits, end_points

batch_size = 32
height, width = 299,299
inputs = tf.random_uniform((batch_size,height,width,3))
with slim.arg_scope(inception_v3_arg_scope()):
    logits, end_points = inception_v3(inputs,is_training = False)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
num_batches = 100
time_tensorflow_run(sess,logits,'Forward')



#======================================================

import collections
import tensorflow as tf
import tensorflow.contrib.slim as slim


class Block(collections.namedtuple('Block',['scope','unit_fn','args'])):
    'A named tuple describing a ResNet block.'
def subsample(inputs,factor,scope = None):
    if factor == 1:
        return inputs
    else:
        return slim.max_pool2d(inputs,[1,1],stride = factor,scope = scope)
        
def conv2d_same(inputs,num_outputs,kernel_size,stride,scope=None):
    if stride == 1:
        return slim.conv2d(inputs,num_outputs,kernel_size,stride = 1,
                           padding = 'SAME',scope = scope)
    else:
        pad_total = kernel_size - 1
        pad_beg = pad_total // 2
        pad_end = pad_total - pad_beg
        inputs= tf.pad(inputs,[[0,0],[pad_beg,pad_end],
                               [pad_beg,pad_end],[0,0]])
        return slim.conv2d(inputs,num_outputs,kernel_size,stride = stride,padding='VALID',scope=scope)
    
@slim.add_arg_scope
def stack_blocks_dense(net,blocks,outputs_collections = None):
    for block in blocks:
        with tf.variable_scope(block.scope,'block') as sc:
            for i, unit in enumerate(block.args):
                with tf.variable_scope('unit_%d'%(i+1),values=[net]):
                    unit_depth, unit_depth_bottleneck, unit_stride = unit
                    net = block.unit_fn(net,
                                        depth = unit_depth,
                                        depth_bottleneck = unit_depth_bottleneck,
                                        stride = unit_stride)
                net = slim.utils.collect_named_outputs(outputs_collections,sc.name,net)
            return net
def resnet_arg_scope(is_training = True,
                     weight_decay = 0.0001,
                     batch_norm_decay = 0.997,
                     batch_norm_epsilon = 1e-5,
                     batch_norm_scale = True):
    batch_norm_params = {
        'is_training':is_training,
        'decay':batch_norm_decay,
        'epsilon':batch_norm_epsilon,
        'scale':batch_norm_scale,
        'updates_collections':tf.GraphKeys.UPDATE_OPS}
    with slim.arg_scope([slim.conv2d],
                        weights_regularizer = slim.l2_regularizer(weight_decay),
                        weights_initializer = slim.variance_scaling_initializer(),
                        activation_fn = tf.nn.relu,
                        normalizer_fn = slim.batch_norm,
                        normalizer_params = batch_norm_params):
        with slim.arg_scope([slim.batch_norm],
                            **batch_norm_params):
            with slim.arg_scope([slim.max_pool2d],
                                padding = 'SAME') as arg_sc:
                return arg_sc
@slim.add_arg_scope
def bottleneck(inputs,depth,depth_bottleneck,stride,
               outputs_collections = None,scope = None):
    with tf.variable_scope(scope,'bottleneck_v2',[inputs]) as sc:
        depth_in = slim.utils.last_dimension(inputs.get_shape(),min_rank = 4)
        preact = slim.batch_norm(input,activation_fn = tf.nn.relu,scope = 'preact')
        if depth == depth_in:
            shortcut = subsample(inputs,stride,'shortcut')
        else:
            shortcut = slim.conv2d(preact,depth,[1,1],stride=stride,
                                   normalizer_fn = None,activation_fn = None,scope = 'shortcut')
        residual = slim.conv2d(preact,depth_bottleneck,[1,1],stride = 1,scope = 'conv1')
        residual = conv2d_same(residual,depth_bottleneck,3,stride,scope='conv2')
        residual = slim.conv2d(residual,depth,[1,1],stride=1,normalizer_fn=None,
                               activation_fn=None,scope='conv3')
        output = shortcut + residual
        return slim.utils.collect_named_outputs(outputs_collections,sc.name,output)

def resnet_v2(inputs,
              blocks,
              num_classes = None,
              global_pool = True, 
              include_root_block = True,
              reuse = None,
              scope = None):
    with tf.variable_scope(scope,'resnet_v2',[inputs],reuse = reuse) as sc:
        end_points_collection = sc.original_name_scope + '_end_points'
        with slim.arg_scope([slim.conv2d,bottleneck,
                             stack_blocks_dense],
                             outputs_collections = end_points_collection):
            net = inputs
            if include_root_block:
                with slim.arg_scope([slim.conv2d],activation_fn = None,normalizer_fn = None):
                    net = conv2d_same(net,64,7,stride = 2,scope = 'conv1')
                net = slim.max_pool2d(net,[3,3],stride = 2,scope = 'pool1')
            net = stack_blocks_dense(net,blocks)
            net = slim.batch_norm(net,activation_fn = tf.nn.relu,scope = 'postnorm')
            if global_pool:
                net = tf.reduce_mean(net,[1,2],name = 'pool5',keep_dims = True)
            if num_classes  is not None:
                net = slim.conv2d(net,
                                  num_classes,
                                  [1,1],
                                  activation_fn = None,
                                  normalizer_fn = None,
                                  scope = 'logits')
                end_points = slim.utils.convert_collection_to_dict(
                                end_points_collection)
            if num_classes is not None:
                end_points['predictions'] = slim.softmax(net,scope = 'predictions')
            return net, end_points
            
def resnet_v2_50(inputs,
                 num_classes = None,
                 global_pool = True,
                 reuse = None,
                 scope = 'resnet_v2_50'):
    blocks = [
        Block('block1',bottleneck,[(256,64,1)] * 2 + [256,64,2]),
        Block('block2',bottleneck,[(512,128,1)] * 3 + [(512,128,2)]),
        Block('block3',bottleneck,[(1024,256,1)] * 5 + [(1024,256,2)]),
        Block('block4',bottleneck,[(2048,512,1)] * 3)]
    return resnet_v2(inputs,blocks,num_classes,global_pool,
                     include_root_block = True, reuse = reuse, scope = scope)
def resnet_v2_101(inputs,
                  num_classes = None,
                  global_pool = True,
                  reuse = None,
                  scope = 'resnet_v2_101'):
    blocks = [
        Block('block1',bottleneck,[(256,64,1)] * 2 + [(256,64,2)]),
        Block('block2',bottleneck,[(512,128,1)] * 3 + [512,128,2]),
        Block('block3',bottleneck,[(1024,256,1)] * 22 + [(1024,256,2)]),
        Block('block4',bottleneck,[(2048,512,1)] * 3)]
    return resnet_v2(inputs,blocks,num_classes,global_pool,
                     include_root_block = True,reuse = reuse, scope = scope)

def resnet_v2_152(inputs,
                  num_classes=None,
                  global_pool=None,
                  reuse=None,
                  scope='resnet_v2_152'):
    blocks = [
        Block('block1',bottleneck,[(256,64,1)] * 2 + [(256,64,2)]),
        Block('block2',bottleneck,[(512,128,1)] * 7 + [(512,128,2)]),
        Block('block3',bottleneck,[(1024,256,1)] * 35 + [(1024,256,2)]),
        Block('block4',bottleneck,[(2048,512,1)] * 3)]
    return resnet_v2(inputs,blocks,num_classes,global_pool,scope = scope,
                     include_root_block = True,reuse = reuse)

def resnet_v2_200(inputs,
                  num_classes=None,
                  global_pool=True,
                  reuse=None,
                  scope='resnet_v2_200'):
    blocks = [
        Block('block1',bottleneck,[(256,64,1)] * 2 + [(256,64,2)]),
        Block('block2',bottleneck,[(512,128,1)] * 23 + [(512,128,2)]),
        Block('block3',bottleneck,[(1024,256,1)] * 35 + [(1024,256,2)]),
        Block('block4',bottleneck,[(2048,512,1)] * 3)]
    return resnet_v2(inputs,blocks,num_classes,global_pool,
                     include_root_block = True,reuse = reuse,scope = scope)
                     
batch_size = 32
height,width = 224,224
inputs = tf.random_uniform((batch_size,height,width,3))
with slim.arg_scope(resnet_arg_scope(is_training = False)):
    net, end_points = resnet_v2_152(inputs,1000)
    
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
num_batches = 100
time_tensorflow_run(sess,net,"Forward")



#==========================================================
#===================== Word2Vec  CBOW =====================

import collections
import math
import os
import random
import zipfile
import numpy as np
import urllib
import tensorflow as tf
import matplotlib.pyplot as plt

url = 'http://mattmahoney.net/dc/'

def maybe_download(filename,expected_bytes):
    if not os.path.exists(filename):
        filename,_ = urllib.request.urlretrieve(url + filename,filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
        print('Found and verified',filename)
    else:
        print(statinfo.st_size)
        raise Exception('Failed to verify' + filename)
    return filename

filename = maybe_download('text8.zip',31344016)

def read_data(filename):
    with zipfile.ZipFile(filename) as f:
        data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data
    
words = read_data(filename)
print('Data Size',len(words))

vocabulary_size = 50000

def build_dataset(words):
    count = [['UNK',-1]]
    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0
            unk_count += 1
        data.append(index)
    count[0][1] = unk_count
    reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))
    return data,count,dictionary,reverse_dictionary
    
data,count,dictionary,reverse_dictionary = build_dataset(words)

del words
print('Most common words (+UNK)',count[:5])
print('Sample data',data[:10],[reverse_dictionary[i] for i in data[:10]])

data_index = 0
def generate_batch(batch_size,num_skips,skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips <= 2 * skip_window
    batch = np.ndarray(shape=(batch_size),dtype=np.int32)
    labels = np.ndarray(shape=(batch_size,1),dtype=np.int32)
    span = 2 * skip_window + 1
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window
        targets_to_avoid = [skip_window]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0,span - 1)
            targets_to_avoid.append(target)
            batch[i*num_skips + j] = buffer[skip_window]
            labels[i*num_skips + j,0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    return batch,labels

batch,labels = generate_batch(batch_size=8,num_skips=2,skip_window=1)
for i in range(8):
    print(batch[i],reverse_dictionary[batch[i]],'->',labels[i,0],
          reverse_dictionary[labels[i,0]])

batch_size = 128
embedding_size = 128
skip_window = 1
num_skips = 2

valid_size = 16
valid_window = 100
valid_examples = np.random.choice(valid_window,valid_size,replace = False)
num_sampled = 64

graph = tf.Graph()
with graph.as_default():
    train_inputs = tf.placeholder(tf.int32,shape=[batch_size])
    train_labels = tf.placeholder(tf.int32,shape=[batch_size,1])
    valid_dataset = tf.constant(valid_examples,dtype=tf.int32)
    
    with tf.device('/cpu:0'):
        embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))
        embed = tf.nn.embedding_lookup(embeddings,train_inputs)
        nce_weights=tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],
                                                    stddev = 1.0 / math.sqrt(embedding_size)))
        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))
    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,
                                         biases=nce_biases,
                                         labels=train_labels,
                                         inputs=embed,
                                         num_sampled=num_sampled,
                                         num_classes=vocabulary_size))
    optimizer=tf.train.GradientDescentOptimizer(1.0).minimize(loss)
    norm=tf.sqrt(tf.reduce_sum(tf.squre(embeddings),1,keep_dims=True))
    normalized_embeddings=embeddings/norm
    valid_embeddings=tf.nn.embedding_lookup(
        normalized_embeddings,valid_dataset)
    similarity=tf.matmul(
        valid_embeddings,normalized_embeddings,transpose_b=True)
    init=tf.global_variables_initializer()

num_steps = 10001

with tf.Session(graph=graph) as session:
    init.run()
    print('Initialized')
    average_loss = 0
    for step  in range(num_steps):
        batch_inputs,batch_labels=generate_batch(
            batch_size,num_skips,skip_window)
        feed_dict = {train_inputs:batch_inputs,train_labels:batch_labels}
        _,loss_val=session.run([optimizer,loss],feed_dict=feed_dict)
        average_loss += loss_val
        if step % 2000 == 0:
            if step > 0:
                average_loss /= 2000
            print('Average loss at step',step,':',average_loss)
            average_loss = 0
        if step % 10000 == 0:
            sim = similarity.eval()
            for i in range(valid_size):
                valid_word = reverse_dictionary[valid_examples[i]]
                top_k = 8
                nearest = (-sim[i,:]).argsort()[1:top_k+1]
                log_str = 'Nearest to %s:' % valid_word
                for k in range(top_k):
                    close_word = reverse_dictionary[nearest[k]]
                    log_str = "%s%s"%(log_str,close_word)
                print(log_str)
    final_embeddings = normalized_embeddings.eval()

def plot_with_labels(low_dim_embs,labels,filename='tsne.png'):
    assert low_dim_embs.shape[0] >= len(labels),"More labels than embeddings"
    plt.figure(figsize=(18,18))
    for i, label in enumerate(labels):
        x,y = low_dim_embs[i:]
        plt.scatter(x,y)
        plt.annotate(label,
                     xy=(x,y),
                     xytext=(5,2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.savefig(filename)

from sklearn.maniford import TSNE
tsne = TSNE(perplexity=30,n_component=2,init='pca',n_iter=5000)
plot_only=100
low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])
labels = [reverse_dictionary[i] for i in range(plot_only)]
plot_with_labels(low_dim_embs,labels)


#=====================================================
#==================== LSTM ===========================
'''
# get and decompress data
wget http://www.fit.vutbr.cz/~imikolov/rcnnlm/simple-exampls.tgz
tar xvf simple-examples.tgz
git clone https://github.com/tensorflow/models.git
cd models/tutorials/rnn/ptb
'''

import time
import numpy as np
import tensorflow as tf
import reader

class PTBInput(object):
    def __init__(self,config,data,name=None):
        self.batch_size = batch_size = config.batch_size
        self.num_steps = num_steps = config.num_steps
        self.epoch_size = ((len(data)//batch_size) - 1) // num_steps
        self.input_data, self.targets = reader.ptb_producer(
            data,batch_size,num_steps,name=name)
            
class PTBModel(object):
    def __init__(self,is_training,config,input_):
        self._input = input_
        batch_size = input_.batch_size
        num_steps = input_.num_steps
        size = config.hidden_size
        vocab_size = config.vocab_size
        def lstm_cell():
            return tf.contrib.rnn.BasicLSTMCell(
                size,forget_bias=0.0,state_is_tuple=True)
        attn_cell = lstm_cell
        if is_training and config.keep_prob < 1:
            def attn_cell():
                return tf.contrib.rnn.DropoutWrappe(
                    lstm_cell(),output_keep_prob = config.keep_prob)
        cell = tf.contrib.rnn.MultiRNNCell(
                    [attn_cell() for _ in range(config.num_layers)],
                    state_is_tuple = True)
        self._initial_state = cell.zero_state(batch_size,tf.float32)
        with tf.device("/cpu:0"):
            embedding = tf.get_variable(
                "embedding",[vocab_size,size],dtype=tf.float32)
            inputs = tf.nn.embedding_lookup(embedding,input_.input_data)
        if is_training and config.keep_prob < 1:
            inputs = tf.nn.dropout(inputs,config.keep_prob)
        outputs = []
        state = self._initial_state
        with tf.variable_scope('RNN'):
            for time_step in range(num_steps):
                if time_step > 0: 
                    tf.get_variable_scope().reuse_variables()
                (cell_output,state) = cell(inputs[:,time_step,:],state)
                outputs.append(cell_output)
        output = tf.reshape(tf.concat(outputs,1),[-1,size])
        softmax_w = tf.get_variable(
            "softmax_w",[size,vocab_size],dtype=tf.float32)
        softmax_b = tf.get_variable(
            "softmax_b",[vocab_size],dtype = tf.float32)
        logits = tf.matmul(output,softmax_w) + softmax_b
        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
            [logits],
            [tf.reshape(input_.targets,[-1])],
            [tf.ones([batch_size*num_steps],dtype=tf.float32)])
        self._cost = cost = tf.reduce_sum(loss) / batch_size
        self._final_state = state
        
        if not is_training:
            return
        self._lr = tf.Variable(0.0,trainable = False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(cost,tvars),
                                          config.max_grad_norm)
        optimizer = tf.train.GradientDescentOptimizer(self._lr)
        self._train_op = optimizer.apply_gradient(zip(grads,tvars),
                            global_step = tf.contrib.framework.get_or_create_global_step())
        self._new_lr = tf.placeholder(
            tf.float32,shape=[],name='new_learning_rate')
        self._lr_update = tf.assign(self._lr,self._new_lr)
    def assign_lr(self,session,lr_value):
        session.run(self._lr_update,feed_dict = {self._new_lr:lr_value})
    
    @property
    def input(self):
        return self._input
    @property
    def initial_state(self):
        return self._initial_state
    @property
    def cost(self):
        return self._cost
    @property
    def final_state(self):
        return self._final_state
    @property
    def lr(self):
        return self._lr
    @property
    def train_op(self):
        return self._train_op
    
class SmallConfig(object):
    init_scale = 0.1
    learning_rate = 1.0
    max_grad_norm = 5
    num_layers = 2
    num_steps = 20
    hidden_size = 200
    max_epoch = 4
    max_max_epoch = 13
    keep_prob = 1.0
    lr_decay = 0.5
    batch_size = 20
    vocab_size = 10000

class MediumConfig(object):
    init_scale = 0.05
    learning_rate = 1.0
    max_grad_norm = 5
    num_layers = 2
    num_steps = 35
    hidden_size = 650
    max_epoch = 6
    max_max_epoch = 39
    keep_prob = 0.5
    lr_decay = 0.8
    batch_size = 20
    vocab_size = 10000

class LargeConfig(object):
    init_scale = 0.04
    learning_rate = 1.0
    max_grad_norm = 10
    num_layers = 2
    num_steps = 35
    hidden_size = 1500
    max_epoch = 14
    max_max_epoch = 55
    keep_prob = 0.35
    lr_decay = 1 / 1.15
    batch_size = 20
    vocab_size = 10000
    
class TestConfig(object):
    init_scale = 0.1
    learning_rate = 1.0
    max_grad_norm = 1
    num_layers = 1
    num_steps = 2
    hidden_size = 2
    max_epoch = 1
    max_max_epoch = 1
    keep_prob = 1.0
    lr_decay = 0.5
    batch_size = 20
    vocab_size = 10000
    
def run_epoch(session,model,eval_op=None,verbose=False):
    start_time = time.time()
    costs = 0.0
    iters = 0
    state = session.run(model.initial_state)
    fetches = {
        "cost":model.cost,
        "final_state":model.final_state}
    if eval_op is not None:
        fetches['eval_op'] = eval_op
    for step in range(model.input.epoch_size):
        feed_dict = {}
        for i, (c,h) in enumerate(model.initial_state):
            feed_dict[c] = state[i].c
            feed_dict[h] = state[i].h
        vals = session.run(fetches,feed_dict)
        cost = vals["cost"]
        state = vals["fianl_state"]
        cost += cost
        iters += model.input.num_steps
        if verbose and step % (model.input.epoch_size // 10) == 10:
            print("%.3f perplexity: %.3f speed:%.0f wps"%
            (step * 1.0 / model.input.epoch_size,np.exp(costs/iters),
             iters * model.input.batch_size / (time.time()- start_time)))
    return np.exp(costs/iters)
    
raw_data = reader.ptb_raw_data('simple-examples/data')
train_data,valid_data,test_data,_ = raw_data
config = SmallConfig()
eval_config = SmallConfig()
eval_config.batch_size = 1
eval_config.num_steps = 1

with tf.Graph().as_default():
    initializer = tf.random_uniform_initializer(-config.init_scale,config.init_scale)
    with tf.name_scope("Train"):
        train_input = PTBInput(config = config,data = train_data,name = "TrainInput")
        with tf.variable_scope("Model",reuse = None,initializer = initializer):
            m = PTBInput(is_training = True,config = config, input_ = train_input)
    with tf.name_scope('valid'):
        valid_input = PTBInput(config = config, data = valid_data,name = 'ValidInput')
        with tf.variable_scope('Model',reuse = True,initializer = initializer):
            mvalid = PTBModel(is_training=False,config=config,input_=valid_input)
    with tf.name_scope("Test"):
        test_input = PTBModel(config=eval_config,data=test_data,
                              name="TestInput")
        with tf.variable_score("Model",reuse=True,initializer=initializer):
            mtest = PTBInput(is_training = False,config = eval_config,
                             input_ = test_input)
sv = tf.train.Supervisor()
with sv.managed_session() as session:
    for i in range(config.max_max_epoch):
        lr_decay = config.lr_decay ** max(i+1-config.max_epoch,0.0)
        m.assign_lr(session,config.learning_rate * lr_decay)
        print("Epoch: %d Learning rate:%.3f"%(i+1,session.run(m.lr)))
        train_perplexity = run_epoch(session,m,eval_op = m.train_op,verbose = True)
        print("Epoch: %d Train Perplexity: %.3f"%(i+1,train_perplexity))
        valid_perplexity = run_epoch(session,mvalid)
        print("Epoch: %d Valid Perplexity: %.3f"%(i+1,valid_perplexity))
        
    test_perplexity = run_epoch(session,mtest)
    print("Test Perplexity: %.3f" % test_perplexity)





    
#================== LSTM Classifier ====================
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/",one_hot = True)

learning_rate = 0.01
max_samples = 400000
batch_size = 128
display_step = 10

n_input = 28
n_steps = 28
n_hidden = 256
n_classes = 10

x = tf.placeholder("float",[None,n_steps,n_input])
y = tf.placeholder("float",[None,n_classes])

weights = tf.Variable(tf.random_normal([2*n_hidden,n_classes]))
biases = tf.Variable(tf.random_normal([n_classes])) 

def BiRNN(x,weights,biases):
    x = tf.transpose(x,[1,0,2])
    x = tf.reshape(x,[-1,n_input])
    x = tf.split(x,n_steps)
    
    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden,forget_bias=1.0)
    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden,forget_bias=1.0)
    outputs,_,_ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell,lstm_bw_cell,x,dtype = tf.float32)
    return tf.matmul(outputs[-1],weights) + biases
    
pred = BiRNN(x,weights,biases)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))
optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

correct_pred=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    step = 1
    while step * batch_size < max_samples:
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        batch_x = batch_x.reshape((batch_size,n_steps,n_input))
        sess.run(optimizer,feed_dict={x:batch_x,y:batch_y})
        if step % display_step == 0:
            acc = sess.run(accuracy, feed_dict={x:batch_x,y:batch_y})
            loss = sess.run(cost,feed_dict={x:batch_x,y:batch_y})
            print("Iter "+str(step*batch_size)+",Minibatch Loss="+"{:.6f}".format(loss) + \
            ",Training Accuracy = " + \
            "{:.5f}".format(acc))
        step += 1
    print("Optimization Finished")
    
    test_len = 10000
    test_data = mnist.test.images[:test_len].reshape((-1,n_steps,n_input))
    test_label = mnist.test.labels[:test_len]
    print("Testing Accuracy:",sess.run(accuracy,feed_dict={x:test_data,y:test_label}))
    






#========================= Policy Network ================
'''
pip install gym
'''


import numpu as np
import tensorflow as tf
import gym
env = gym.make('CartPole-v0')

env.reset()
random_episodes = 0
reward_sum = 0
while random_episodes < 10:
    env.render()
    observation,reward,done,_ = env.step(np.random.randint(0,2))
    reward_sum += reward
    if done:
        random_episodes += 1
        print("Reward for this episode was:",reward_sum)
        reward_sum = 0
        env.reset()

H = 50
batch_size = 25
learning_rate = 1e-1
D = 4
gamma = 0.99

observations = tf.placeholder(tf.float32,[None,D],name = "input_x")
w1 = tf.get_variable('w1',shape=[D,H],initializer = tf.contrib.layers.xavier_initializer())
layer1 = tf.nn.relu(tf.matmul(observations,w1))
w2 = tf.get_variable('w2',shape=[H,1],initializer=tf.contrib.layers.xavier_initializer())
score = tf.matmul(layer1,w2)
probability = tf.nn.sigmoid(score)

adam = tf.train.AdamOptimizer(learning_rate = learning_rate)
w1grad = tf.placeholder(tf.float32,name='batch_grad1')
w2grad = tf.placeholder(tf.float32,name='batch_grad2')
batchGrad = [w1grad,w2grad]
updateGrads = adam.apply_gradients(zip(batchGrad,tvars))

def discount_rewards(r):
    discounted_r = np.zeros_like(r)
    running_add = 0
    for t in reversed(range(r.size)):
        running_add = running_add * gamma + r[t]
        discounted_r[t] = running_add
    return discounted_r
    
input_y = tf.placeholder(tf.float32,[None,1],name='input_y')
advantages = tf.placeholder(tf.float32,name='reward_signal')
loglik = tf.log(input_y * (input_y - probability) + \
                (1-input_y)*(input_y + probability))
loss = -tf.reduce_mean(loglik*advantages)

tvars = tf.trainable_variables()
newGrads = tf.gradient(loss,tvars)

xs,ys,drs = [],[],[]
reward_sum = 0
episode_number = 1
total_episode = 10000

with tf.Session() as sess:
    rendering = False
    init = tf.global_variables_initializer()
    sess.run(init)
    observation = env.reset()
    
    gradBuffer = sess.run(tvars)
    for ix,grad in enumerate(gradBuffer):
        gradBuffer[ix] = grad * 0
    while episode_number <= total_episode:
        if reward_sum/batch_size > 100 or rendering == True:
            env.render()
            rendering = True
        x = np.reshape(observation,[1,D])
        tfprob = sess.run(probability,feed_dict)
        action = 1  if np.random.uniform() < tfprob else 0
        
        xs.append(x)
        y = 1 - action
        ys.append(y)
        observation,reward,done,info = env.step(action)
        reward_sum += reward
        drs.append(reward)
        if done:
            episode_number += 1
            epx = np.vstack(xs)
            epy = np.vstack(ys)
            epr = np.vstack(drs)
            xs,ys,drs = [],[],[]
            discounted_epr = discount_rewards(epr)
            discounted_epr -= np.mean(discounted_epr)
            discounted_epr /= np.std(discounted_epr)
            tGrad = sess.run(newGrads,feed_dict={observations:epx,
                                                 input_y:epy,
                                                 advantages:discounted_epr})
            for ix,grad in enumerate(tGrad):
                gradBuffer[ix] += grad
            if episode_number % batch_size == 0:
                sess.run(updateGrads,feed_dict={w1grad:gradBuffer[0],
                                                w2grad:gradBuffer[1]})
                for ix,grad in enumerate(gradBuffer):
                    gradBuffer[ix] = grad * 0
                    
                print("Average reward for episode %d:%f."%\
                    (episode_number,reward_sum/batch_size))
                if reward_sum/batch_size > 200:
                    print("Task solved in",episode_number,'episodes!')
                    break
                reward_sum = 0
            observation = env.reset()
            
            
        
#================ Value Network ===================
        
import numpy as np
import random
import itertools
import scipy.misc
import matplotlib.pyplot as plt
import tensorflow as tf
import os

class gameOb():
    def __init__(self,coordinates,size,intensity,channel,reward,name):
        self.x = coordinates[0]
        self.y = coordinates[1]
        self.size = size
        self.intensity = intensity
        self.channel = channel
        self.reward = reward
        self.name = name
		
class gameEnv():
    def __init__(self,size):
        self.sizeX = size
        self.sizeY = size
        self.actions = 4
        self.objects = []
        a = self.reset()
        plt.imshow(a,interpolation = "nearest")
		
    def reset(self):
        self.objects = []
        hero = gameOb(self.newPostition(),1,1,2,None,'hero')
        self.objects.append(hero)
        goal = gameOb(self.newPostition(),1,1,1,1,'goal')
        self.objects.append(goal)
        hole = gameOb(self.newPostition(),1,1,0,-1,'fire')
        self.objects.append(hole)
        goal2 = gameOb(self.newPostition(),1,1,1,1,'goal')
        self.objects.append(goal2)
        hole2 = gameOb(self.newPostition(),1,1,0,-1,'fire')
        self.objects.append(hole2)
        goal3 = gameOb(self.newPostition(),1,1,1,1,'goal')
        self.objects.append(goal3)
        goal4 = gameOb(self.newPostition(),1,1,1,1,'goal')
        self.objects.append(goal4)
        state = self.renderEnv()
        self.state = state
        return state
		
    def moveChar(self,direction):
        hero = self.objects[0]
        heroX = hero.x
        heroY = hero.y
        if direction == 0 and hero.y >= 1:
            hero.y -= 1
        if direction == 1 and hero.y <= self.sizeY - 2:
            hero.y += 1
        if direction == 2 and hero.x >= 1:
            hero.x -= 1
        if direction == 3 and hero.x <= self.sizeX - 2:
            hero.x += 1
        self.objects[0] = hero
		
    def newPostition(self):
        iterables = [range(self.sizeX),range(self.sizeY)]
        points = []
        for t in itertools.product(*iterables):
            points.append(t)
            currentPositions = []
        for objectA in self.objects:
            if (objectA.x,objectA.y) not in currentPositions:
                currentPositions.append((objectA.x,objectA.y))
        for pos in currentPositions:
            points.remove(pos)
        location = np.random.choice(range(len(points)),replace=False)
        return points[location]
    
    def checkGoal(self):
        others = []
        for obj in self.objects:
            if 'hero' == obj.name:
                hero = obj
            else:
                others.append()
        for other in others:
            if hero.x == other.x and hero.y == other.y:
                self.objects.remove(other)
                if 1 == other.reward:
                    self.objects.append(gameOb(self.newPostition(),1,1,1,1,'goal'))
                else:
                    self.objects.append(gameOb(self.newPostition(),1,1,0,-1,'fire'))
        return 0.0,False
        
    def renderEnv(self):
        a = np.ones([self.sizeX + 2,self.sizeY + 2,3])
        a[1:-1,1:-1,:] = 0
        hero = None
        for item in self.objects:
            a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity
        b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')
        c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest') 
        d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest')
        a = np.stack([b,c,d],axis=2)
        return a
        
    def step(self,action):
        self.moveChar(action)
        reward,done = self.checkGoal()
        state = self.renderEnv()
        return state,reward,done
    
env = gameEnv(size = 5)

class Qnetwork():
    def __init__(self,h_size):
        self.scalarInput = tf.placeholder(shape=[None,21168],
                                          dtype=tf.float32)
        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])
        self.conv1 = tf.contrib.layers.convolution2d(
            inputs = self.imageIn,num_outputs=32,
            kernel_size=[8,8],stride=[4,4],
            padding='VALID',biases_initializer=None)
        self.conv2 = tf.contrib.layers.convolution2d(
            inputs = self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],
            padding='VALID',biases_initializer=None)
        self.conv3 = tf.contrib.layer.convolution2d(
            inputs = self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],
            padding='VALID',biases_initializer=None)
        self.conv4 = tf.contrib.layers.conv2d(
            inputs=self.conv3,num_outputs=512,
            kernel_size=[7,7],stride=[1,1],
            padding='VALID',biases_initializer=None)
        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)
        self.streamA = tf.contrib.layers.flatten(self.streamAC)
        self.streamV = tf.contrib.layers.flatten(self.streamVC)
        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))
        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))
        self.Advantage = tf.matmul(self.streamA,self.AW)
        self.Value = tf.matmul(self.streamV,self.VW)
        
        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(
            self.Advantage,reduction_indices=1,keep_dims=True))
        self.predict = tf.argmax(self.Qout,1)
        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)
        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)
        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)
        self.Q = tf.reduce_sum(tf.multiply(self.Qout,self.actions_onehot),reduction_indices=1)
        self.td_error = tf.square(self.targetQ-self.Q)
        self.loss = tf.reduce_mean(self.td_error)
        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)
        self.updateModel = self.trainer.minimize(self.loss)

class experience_buffer():
    def __init__(self,buffer_size = 50000):
        self.buffer = []
        self.buffer_size = buffer_size
    def add(self,experience):
        if len(self.buffer) + len(experience) >= self.buffer_size:
            self.buffer[0:(len(experience) + len(self.buffer) - self.buffer_size)] = []
            self.buffer.extend(experience)
    def sample(self,size):
        return np.reshape(np.array(random.sample(self.buffer_size)),[size,5])
    
def processState(states):
    return np.reshape(states,[21168])

def updateTargetGraph(tfVars,tau):
    total_vars = len(tfVars)
    op_holder = []
    for idx,var in enumerate(tfVars[0:total_vars//2]):
        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau)+\
            ((1-tau)*tfVars[idx+total_vars//2].value())))
    return op_holder
    
def updateTarget(op_holder,sess):
    for op in op_holder:
        sess.run(op)

batch_size = 32
update_freq = 4
y = .99
startE = 1
endE = 0.1
annealing_steps = 10000
num_episodes = 10000
pre_train_steps = 10000
max_epLength = 50
load_model = False
path = "./dqn"
h_size = 512
tau = 0.001

mainQN = Qnetwork(h_size)
targetQN = Qnetwork(h_size)
init = tf.global_variables_initializer()

trainables = tf.trainable_variables()
targetOps = updateTargetGraph(trainables,tau)

mybuffer = experience_buffer()
e = startE
stepDrop = (startE - endE) / annealing_steps

rList = []
total_steps = 0
saver = tf.train.Saver()
if not os.path.exists(path):
    os.makedirs(path)

with tf.Session() as sess:
    if load_model == True:
        print("loading model...")
        ckpt = tf.train.get_checkpoint_state(path)
        saver.restore(sess,ckpt.model_checkpoint_path)
    saver.run(init)
    updateTarget(targetOps,sess)
    for i in range(num_episodes+1):
        episodeBuffer = experience_buffer()
        s = env.reset()
        s = processState(s)
        d = False
        rAll = 0
        j = 0
        while j < max_epLength:
            j += 1
            if np.random.rand(1) < e or total_steps < pre_train_steps:
                a = np.random.randint(0,4)
            else:
                a = sess.run(mainQN.predict,
                             feed_dict = {mainQN.scalarInput:[s]})[0]
            s1,r,d = env.step(a)
            s1 = processState(s1)
            total_steps += 1
            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))
            if total_steps > pre_train_steps:
                if e > endE:
                    e -= stepDrop
            if total_steps % (update_freq) == 0:
                trainBatch = mybuffer.sample(batch_size)
                A = sess.run(mainQN.predict,feed_dict={
                    mainQN.scalarInput:np.vstack(trainBatch[:,3])})
                Q = sess.run(targetQN.Qout,feed_dict={
                    targetQN.scalarInput:np.vstack(trainBatch[:,3])})
                doubleQ = Q[range(batch_size),A]
                targetQ = trainBatch[:,2] + y*doubleQ
                _ = sess.run(mainQN.updateModel,feed_dict={
                    mainQN.scalarInput:np.vstack(trainBatch[:,0]),
                    mainQN.targetQ:targetQ,
                    mainQN.actions:trainBatch[:,1]})
                updateTarget(targetOps,sess)
            rAll += r
            s = s1
            if d == True:
                break
        mybuffer.add(episodeBuffer.buffer)
        rList.append(rAll)
        if i > 0 and i % 25 ==0:
            print('episode',i,', average reward of last 25 episode',
                  np.mean(rList[-25:]))
        if i > 0 and i % 1000 == 0:
            saver.save(sess,path+'/model-'+str(i)+'.cptk')
            print("save model")
    saver.save(sess,path+'/model-',str(i)+'.cptk')

rMat = np.resize(np.array(rList),[len(rList)//100,100])
rMean = np.average(rMat,1)
plt.plot(rMean)



#============================= Tensorboard ==================

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
max_steps = 1000
learning_rate = 0.001
dropout = 0.9
data_dir = '/tmp/tensorflow/mnist/input_data'
log_dir = '/tmp/tensorflow/mnist/logs/mnist_with_summaries'

mnist = input_data.read_data_sets(data_dir,one_hot = True)
sess = tf.InteractiveSession()

with tf.name_scope('input'):
    x = tf.placeholder(tf.float32,[None,784],name = 'x-input')
    y_ = tf.placeholder(tf.float32,[None,10],name = 'y-input')
with tf.name_scope('input_reshape'):
    image_shaped_input = tf.reshape(x,[-1,28,28,1])
    tf.summary.image('input',image_shaped_input,10)

def weight_variable(shape):
    initial = tf.truncated_normal(shape,stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.Constant(0.1,shape=shape)
    return tf.Variable(initial)
    
def variable_summaries(var):
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean',mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
        tf.summary.scalar('stddev',stddev)
        tf.summary.scalar('max',tf.reduce_max(var))
        tf.summary.scalar('min',tf.reduce_min(var))
        tf.summary.histogram('histogram',var)

def nn_layer(input_tensor,input_dim,output_dim,layer_name,act=tf.nn.relu):
    with tf.name_scope(layer_name):
        with tf.name_scope('weights'):
            weights = weight_variable([input_dim,output_dim])
            variable_summaries(weights)
        with tf.name_scope('biases'):
            biases = bias_variable([output_dim])
            variable_summaries(biases)
        with tf.name_scope('wx_plus_b'):
            preactivate = tf.nn.bias_add(tf.matmul(input_tensor,weights),biases)
            tf.summary.histogram('pre_activations',preactivate)
        activations = act(preactivate,name='activation')
        tf.summary.histogram('activations',activations)
        return activations
        
hidden1 = nn_layer(x,784,784,'layer_1')
with tf.name_scope('dropout'):
    keep_prob = tf.placeholder(tf.float32)
    tf.summary.scalar('dropout_rate',keep_prob)
    dropped = tf.nn.dropout(hidden1,keep_prob)
y = nn_layer(dropped,500,10,'layer_2',act=tf.identity)

with tf.name_scope('cross_entropy'):
    diff = tf.nn.softmax_cross_entropy_with_logits(logits=y,lables=y)
    with tf.name_scope('total'):
        cross_entropy = tf.reduce_mean(diff)
tf.summary.scalar('cross_entropy',cross_entropy)

with tf.name_scope('train'):
    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)
with tf.name_scope('accracy'):
    with tf.name_scope('correct_prediction'):
        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))
    with tf.name_scope('accuracy'):
        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
tf.summary.scalar('accuray',accuracy)

merged = tf.summary.merge_all()
train_writer = tf.summary.FileWriter(log_dir+'/train',sess.graph)
test_writer = tf.summary.FileWriter(log_dir+'/test')
tf.global_variables_initializer().run()

def feed_dict(train):
    if train:
        xs,ys = mnist.train.next_batch(100)
        k = dropout
    else:
        xs,ys = mnist.test.images,mnist.test.labels
        k = 1.0
    return {x:xs,y_:ys,keep_prob:k}

saver = tf.train.Saver()
for i in range(max_steps):
    if i%10 == 0:
        summary,acc = sess.run([merged,accuracy],feed_dict=feed_dict(False))
        test_writer.add_summary(summary,i)
        print('Accuracy at step %s:%s'%(i,acc))
    else:
        if i%100 == 99:
            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
            run_metadata = tf.RunMetadata()
            summary,_ = sess.run([merged,train_step],
                                 feed_dict=feed_dict(True),
                                 options = run_options,
                                 run_metadata=run_metadata)
            train_writer.add_run_metadata(run_metadata,'step%03d'%i)
            train_writer.add_summary(summary,i)
            saver.save(sess,log_dir+"/model.ckpt",i)
            print('Adding run metadata for',i)
        else:
            summary,_ = sess.run([merged,train_step],feed_dict=feed_dict(True))
            train_writer.add_summary(summary,i)
train_writer.close()
test_writer.close()

#tensorboard --logdir=/tmp/tensorflow/mnist/logs/mnist_with_summaries

            
            
#========================== multiple gpu ==========================
import os.path
import re
import time
import numpy as np
import tensorflow as tf
import cifar10

batch_size = 128
max_steps = 1000000
num_gpus = 4

def tower_loss(scope):
    images,labels = cifar10.distorted_inputs()
    logits = cifar10.inference(images)
    _ = cifar10.loss(logits,labels)
    losses = tf.get_collections('losses',scope)
    total_loss = tf.add_n(losses,name='total_loss')
    return total_loss

def average_gradient(tower_grads):
    average_grads = []
    for grad_and_vars in zip(*tower_grads):
        grads = []
        for g,_ in grad_and_vars:
            expanded_g = tf.expand_dims(g,0)
            grads.append(expanded_g)
        
        grad = tf.concat(grads,0)
        grad = tf.reduce_mean(grad,0)
        v = grad_and_vars[0][1]
        grad_and_var = (grad,v)
        average_grads.append(grad_and_var)
    return average_grads
    
def train():
    with tf.Graph().as_default(),tf.device('/cpu:0'):
        global_step = tf.get_variable('global_step',[],
                                        initializer=tf.constant_initializer(0),
                                        trainable=False)
        num_batches_per_epoch = cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN/batch_size
        decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)
        lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,
                                        global_step,
                                        decay_steps,
                                        cifar10.LEARNING_RATE_DECAY_FACTOR,
                                        staircase=True)
        opt = tf.train.GradientDescentOptimizer(lr)
        tower_grads = []
        for i in range(num_gpus):
            with tf.device('/gpu:%d'%i):
                with tf.name_scope('%s_%d'%(cifar10.TOWER_NAME,i)) as scope:
                    loss = tower_loss(scope)
                    tf.get_variable_scope().reuse_variables()
                    grads = opt.compute_gradients(loss)
                    tower_grads.append(grads)
        grads = average_gradient(tower_grads)
        apply_gradient_op = opt.apply_gradients(grads,global_step=global_step)
        saver = tf.train.Saver(tf.all_variables())
        init = tf.global_variables_initialzer()
        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))
        sess.run(init)
        tf.train.start_queue_runners(sess=sess)
        for step in range(max_steps):
            start_time = time.time()
            _,loss_value = sess.run([apply_gradient_op,loss])
            duration = time.time() - start_time
            if step % 10 == 0:
                num_examples_per_step = batch_size * num_gpus
                examples_per_sec = num_examples_per_step / duration
                sec_per_batch = duration / num_gpus
                
                format_str = ('step%d,loss=%.2f(%.1f examples/sec);%.3fsec/batch')
                print(format_str%(step,loss_value,examples_per_sec,sec_per_batch))
            if step % 1000 == 0 or (step+1) == max_steps:
                saver.save(sess,'/tmp/cifar10_train/model.ckpt',global_step=step)
cifar10.maybe_download_and_extract()

train()

















